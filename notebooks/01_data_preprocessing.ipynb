{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n",
      "Pandas version: 2.3.3\n",
      "Numpy version: 2.3.5\n",
      "Current time: 2025-12-06 13:00:51\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "print(f\"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Define File Paths and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ROBUST DATA LOADING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "=== ANALYZING FILE STRUCTURE ===\n",
      "\n",
      "1. Checking file encoding...\n",
      "‚úì Encoding 'utf-8' works\n",
      "\n",
      "2. Analyzing first 5 lines...\n",
      "First 5 lines of the file:\n",
      "Line 1: UnderwrittenCoverID|PolicyID|TransactionMonth|IsVATRegistered|Citizenship|LegalType|Title|Language|B...\n",
      "Line 2: 145249|12827|2015-03-01 00:00:00|True|  |Close Corporation|Mr|English|First National Bank|Current ac...\n",
      "Line 3: 145249|12827|2015-05-01 00:00:00|True|  |Close Corporation|Mr|English|First National Bank|Current ac...\n",
      "Line 4: 145249|12827|2015-07-01 00:00:00|True|  |Close Corporation|Mr|English|First National Bank|Current ac...\n",
      "Line 5: 145255|12827|2015-05-01 00:00:00|True|  |Close Corporation|Mr|English|First National Bank|Current ac...\n",
      "\n",
      "3. Detecting separator...\n",
      "‚úì Detected separator: '|' (escaped: '|')\n",
      "\n",
      "4. Scanning for problematic lines...\n",
      "Line 1: 52 fields\n",
      "Line 2: 52 fields\n",
      "Line 3: 52 fields\n",
      "Line 4: 52 fields\n",
      "Line 5: 52 fields\n",
      "Line 6: 52 fields\n",
      "Line 7: 52 fields\n",
      "Line 8: 52 fields\n",
      "Line 9: 52 fields\n",
      "Line 10: 52 fields\n",
      "\n",
      "Most common field count: 52\n",
      "\n",
      "================================================================================\n",
      "LOADING DATA WITH ERROR HANDLING\n",
      "================================================================================\n",
      "\n",
      "Option 1: Loading with on_bad_lines='skip'...\n",
      "‚úì Successfully loaded with skipping bad lines\n",
      "  Shape: 1,000,098 rows √ó 52 columns\n",
      "\n",
      "================================================================================\n",
      "BASIC DATA INSPECTION\n",
      "================================================================================\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Shape: 1,000,098 rows √ó 52 columns\n",
      "\n",
      "First 5 rows:\n",
      "   UnderwrittenCoverID  PolicyID     TransactionMonth  IsVATRegistered Citizenship          LegalType Title Language                 Bank      AccountType  MaritalStatus         Gender       Country Province  PostalCode MainCrestaZone SubCrestaZone          ItemType      mmcode        VehicleType  RegistrationYear           make  Model  Cylinders  cubiccapacity  kilowatts bodytype  NumberOfDoors VehicleIntroDate  CustomValueEstimate AlarmImmobiliser TrackingDevice CapitalOutstanding          NewVehicle WrittenOff Rebuilt Converted CrossBorder  NumberOfVehiclesInFleet  SumInsured TermFrequency  CalculatedPremiumPerTerm                    ExcessSelected CoverCategory   CoverType            CoverGroup              Section                          Product StatutoryClass StatutoryRiskType  TotalPremium  TotalClaims\n",
      "0               145249     12827  2015-03-01 00:00:00             True              Close Corporation    Mr  English  First National Bank  Current account  Not specified  Not specified  South Africa  Gauteng        1459      Rand East     Rand East  Mobility - Motor 44069150.00  Passenger Vehicle              2004  MERCEDES-BENZ  E 240       6.00        2597.00     130.00      S/D           4.00           6/2002            119300.00              Yes             No             119300  More than 6 months        NaN     NaN       NaN         NaN                      NaN        0.01       Monthly                     25.00             Mobility - Windscreen    Windscreen  Windscreen  Comprehensive - Taxi  Motor Comprehensive  Mobility Metered Taxis: Monthly     Commercial     IFRS Constant         21.93         0.00\n",
      "1               145249     12827  2015-05-01 00:00:00             True              Close Corporation    Mr  English  First National Bank  Current account  Not specified  Not specified  South Africa  Gauteng        1459      Rand East     Rand East  Mobility - Motor 44069150.00  Passenger Vehicle              2004  MERCEDES-BENZ  E 240       6.00        2597.00     130.00      S/D           4.00           6/2002            119300.00              Yes             No             119300  More than 6 months        NaN     NaN       NaN         NaN                      NaN        0.01       Monthly                     25.00             Mobility - Windscreen    Windscreen  Windscreen  Comprehensive - Taxi  Motor Comprehensive  Mobility Metered Taxis: Monthly     Commercial     IFRS Constant         21.93         0.00\n",
      "2               145249     12827  2015-07-01 00:00:00             True              Close Corporation    Mr  English  First National Bank  Current account  Not specified  Not specified  South Africa  Gauteng        1459      Rand East     Rand East  Mobility - Motor 44069150.00  Passenger Vehicle              2004  MERCEDES-BENZ  E 240       6.00        2597.00     130.00      S/D           4.00           6/2002            119300.00              Yes             No             119300  More than 6 months        NaN     NaN       NaN         NaN                      NaN        0.01       Monthly                     25.00             Mobility - Windscreen    Windscreen  Windscreen  Comprehensive - Taxi  Motor Comprehensive  Mobility Metered Taxis: Monthly     Commercial     IFRS Constant          0.00         0.00\n",
      "3               145255     12827  2015-05-01 00:00:00             True              Close Corporation    Mr  English  First National Bank  Current account  Not specified  Not specified  South Africa  Gauteng        1459      Rand East     Rand East  Mobility - Motor 44069150.00  Passenger Vehicle              2004  MERCEDES-BENZ  E 240       6.00        2597.00     130.00      S/D           4.00           6/2002            119300.00              Yes             No             119300  More than 6 months        NaN     NaN       NaN         NaN                      NaN   119300.00       Monthly                    584.65  Mobility - Metered Taxis - R2000    Own damage  Own Damage  Comprehensive - Taxi  Motor Comprehensive  Mobility Metered Taxis: Monthly     Commercial     IFRS Constant        512.85         0.00\n",
      "4               145255     12827  2015-07-01 00:00:00             True              Close Corporation    Mr  English  First National Bank  Current account  Not specified  Not specified  South Africa  Gauteng        1459      Rand East     Rand East  Mobility - Motor 44069150.00  Passenger Vehicle              2004  MERCEDES-BENZ  E 240       6.00        2597.00     130.00      S/D           4.00           6/2002            119300.00              Yes             No             119300  More than 6 months        NaN     NaN       NaN         NaN                      NaN   119300.00       Monthly                    584.65  Mobility - Metered Taxis - R2000    Own damage  Own Damage  Comprehensive - Taxi  Motor Comprehensive  Mobility Metered Taxis: Monthly     Commercial     IFRS Constant          0.00         0.00\n",
      "\n",
      "Column names:\n",
      "  1. UnderwrittenCoverID\n",
      "  2. PolicyID\n",
      "  3. TransactionMonth\n",
      "  4. IsVATRegistered\n",
      "  5. Citizenship\n",
      "  6. LegalType\n",
      "  7. Title\n",
      "  8. Language\n",
      "  9. Bank\n",
      " 10. AccountType\n",
      " 11. MaritalStatus\n",
      " 12. Gender\n",
      " 13. Country\n",
      " 14. Province\n",
      " 15. PostalCode\n",
      " 16. MainCrestaZone\n",
      " 17. SubCrestaZone\n",
      " 18. ItemType\n",
      " 19. mmcode\n",
      " 20. VehicleType\n",
      " 21. RegistrationYear\n",
      " 22. make\n",
      " 23. Model\n",
      " 24. Cylinders\n",
      " 25. cubiccapacity\n",
      " 26. kilowatts\n",
      " 27. bodytype\n",
      " 28. NumberOfDoors\n",
      " 29. VehicleIntroDate\n",
      " 30. CustomValueEstimate\n",
      " 31. AlarmImmobiliser\n",
      " 32. TrackingDevice\n",
      " 33. CapitalOutstanding\n",
      " 34. NewVehicle\n",
      " 35. WrittenOff\n",
      " 36. Rebuilt\n",
      " 37. Converted\n",
      " 38. CrossBorder\n",
      " 39. NumberOfVehiclesInFleet\n",
      " 40. SumInsured\n",
      " 41. TermFrequency\n",
      " 42. CalculatedPremiumPerTerm\n",
      " 43. ExcessSelected\n",
      " 44. CoverCategory\n",
      " 45. CoverType\n",
      " 46. CoverGroup\n",
      " 47. Section\n",
      " 48. Product\n",
      " 49. StatutoryClass\n",
      " 50. StatutoryRiskType\n",
      " 51. TotalPremium\n",
      " 52. TotalClaims\n",
      "\n",
      "Data types:\n",
      "object     36\n",
      "float64    11\n",
      "int64       4\n",
      "bool        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION AGAINST PROJECT SPECIFICATION\n",
      "================================================================================\n",
      "\n",
      "‚úì Columns in project specification: 52\n",
      "‚úì Columns in loaded data: 52\n",
      "\n",
      "üìä MATCH SUMMARY:\n",
      "  ‚Ä¢ Matched columns: 47\n",
      "  ‚Ä¢ Missing columns: 5\n",
      "  ‚Ä¢ Extra columns: 5\n",
      "\n",
      "‚ö†Ô∏è MISSING COLUMNS:\n",
      "  1. Mmcode\n",
      "  2. Make\n",
      "  3. Cubiccapacity\n",
      "  4. Kilowatts\n",
      "  5. Bodytype\n",
      "\n",
      "üìù EXTRA COLUMNS (not in spec):\n",
      "  1. mmcode\n",
      "  2. make\n",
      "  3. cubiccapacity\n",
      "  4. kilowatts\n",
      "  5. bodytype\n",
      "\n",
      "================================================================================\n",
      "SAVING LOADED DATA\n",
      "================================================================================\n",
      "‚úì Saved loaded data to: ../data/processed/raw_loaded_data.parquet\n",
      "  Size: 11.8 MB\n",
      "‚úì Saved sample to: ../data/processed/data_sample.csv\n",
      "‚úì Saved metadata to: ../data/processed/loading_metadata.json\n",
      "\n",
      "================================================================================\n",
      "QUICK DATA QUALITY CHECK\n",
      "================================================================================\n",
      "\n",
      "1. Missing Values:\n",
      "  ‚Ä¢ Bank: 145,961 (14.6%)\n",
      "  ‚Ä¢ AccountType: 40,232 (4.0%)\n",
      "  ‚Ä¢ MaritalStatus: 8,259 (0.8%)\n",
      "  ‚Ä¢ Gender: 9,536 (1.0%)\n",
      "  ‚Ä¢ mmcode: 552 (0.1%)\n",
      "  ‚Ä¢ VehicleType: 552 (0.1%)\n",
      "  ‚Ä¢ make: 552 (0.1%)\n",
      "  ‚Ä¢ Model: 552 (0.1%)\n",
      "  ‚Ä¢ Cylinders: 552 (0.1%)\n",
      "  ‚Ä¢ cubiccapacity: 552 (0.1%)\n",
      "\n",
      "2. Basic Statistics:\n",
      "  ‚Ä¢ Total Premium: R61,911,562.70\n",
      "  ‚Ä¢ Total Claims: R64,867,546.17\n",
      "  ‚Ä¢ Loss Ratio: 104.8%\n",
      "\n",
      "3. Temporal Coverage:\n",
      "  ‚Ä¢ Date Range: 2013-10-01 00:00:00 to 2015-08-01 00:00:00\n",
      "\n",
      "================================================================================\n",
      "NEXT STEPS RECOMMENDATION\n",
      "================================================================================\n",
      "\n",
      "Based on the loaded data, here are the next steps:\n",
      "1. ‚úÖ Data loaded successfully with 47/52 columns\n",
      "2. Proceed with preprocessing pipeline\n",
      "3. Focus on cleaning 5 missing columns\n",
      "\n",
      "üíæ Files created:\n",
      "  ‚Ä¢ Processed data: ../data/processed/raw_loaded_data.parquet\n",
      "  ‚Ä¢ Sample data: ../data/processed/data_sample.csv\n",
      "  ‚Ä¢ Loading metadata: ../data/processed/loading_metadata.json\n",
      "\n",
      "================================================================================\n",
      "READY FOR PREPROCESSING\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ROBUST DATA LOADING PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: ANALYZE FILE STRUCTURE\n",
    "# ============================================================================\n",
    "print(\"\\n=== ANALYZING FILE STRUCTURE ===\")\n",
    "\n",
    "RAW_DATA_PATH = '../data/raw/raw_data.txt'\n",
    "\n",
    "# Check file encoding first\n",
    "print(\"\\n1. Checking file encoding...\")\n",
    "encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']\n",
    "\n",
    "for encoding in encodings:\n",
    "    try:\n",
    "        with open(RAW_DATA_PATH, 'r', encoding=encoding) as f:\n",
    "            f.readline()\n",
    "        print(f\"‚úì Encoding '{encoding}' works\")\n",
    "        FILE_ENCODING = encoding\n",
    "        break\n",
    "    except:\n",
    "        continue\n",
    "else:\n",
    "    FILE_ENCODING = 'utf-8'\n",
    "    print(\"‚ö†Ô∏è Could not detect encoding, using 'utf-8'\")\n",
    "\n",
    "# Analyze first few lines\n",
    "print(\"\\n2. Analyzing first 5 lines...\")\n",
    "with open(RAW_DATA_PATH, 'r', encoding=FILE_ENCODING) as f:\n",
    "    lines = [f.readline().strip() for _ in range(5)]\n",
    "\n",
    "print(\"First 5 lines of the file:\")\n",
    "for i, line in enumerate(lines, 1):\n",
    "    print(f\"Line {i}: {line[:100]}...\")  # Show first 100 chars\n",
    "\n",
    "# Detect separator by analyzing multiple lines\n",
    "print(\"\\n3. Detecting separator...\")\n",
    "separator_counts = {'comma': 0, 'tab': 0, 'semicolon': 0, 'pipe': 0}\n",
    "\n",
    "for line in lines:\n",
    "    comma_count = line.count(',')\n",
    "    tab_count = line.count('\\t')\n",
    "    semicolon_count = line.count(';')\n",
    "    pipe_count = line.count('|')\n",
    "    \n",
    "    if comma_count > 0:\n",
    "        separator_counts['comma'] += 1\n",
    "    if tab_count > 0:\n",
    "        separator_counts['tab'] += 1\n",
    "    if semicolon_count > 0:\n",
    "        separator_counts['semicolon'] += 1\n",
    "    if pipe_count > 0:\n",
    "        separator_counts['pipe'] += 1\n",
    "\n",
    "# Choose the most common separator\n",
    "max_sep = max(separator_counts, key=separator_counts.get)\n",
    "separator_map = {'comma': ',', 'tab': '\\t', 'semicolon': ';', 'pipe': '|'}\n",
    "SEPARATOR = separator_map[max_sep] if separator_counts[max_sep] > 0 else ','\n",
    "\n",
    "print(f\"‚úì Detected separator: '{SEPARATOR}' (escaped: {repr(SEPARATOR)})\")\n",
    "\n",
    "# Count total lines and check problematic lines\n",
    "print(\"\\n4. Scanning for problematic lines...\")\n",
    "line_lengths = []\n",
    "problematic_lines = []\n",
    "\n",
    "with open(RAW_DATA_PATH, 'r', encoding=FILE_ENCODING) as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        if i <= 1000:  # Check first 1000 lines for patterns\n",
    "            fields = line.strip().split(SEPARATOR)\n",
    "            line_lengths.append(len(fields))\n",
    "            if i <= 10:\n",
    "                print(f\"Line {i}: {len(fields)} fields\")\n",
    "\n",
    "# Analyze field count distribution\n",
    "from collections import Counter\n",
    "field_counts = Counter(line_lengths)\n",
    "most_common = field_counts.most_common(1)[0][0]\n",
    "print(f\"\\nMost common field count: {most_common}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: LOAD WITH ERROR HANDLING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LOADING DATA WITH ERROR HANDLING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Option 1: Try loading with error handling\n",
    "print(\"\\nOption 1: Loading with on_bad_lines='skip'...\")\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        RAW_DATA_PATH,\n",
    "        sep=SEPARATOR,\n",
    "        encoding=FILE_ENCODING,\n",
    "        on_bad_lines='skip',\n",
    "        low_memory=False\n",
    "    )\n",
    "    print(f\"‚úì Successfully loaded with skipping bad lines\")\n",
    "    print(f\"  Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Failed: {e}\")\n",
    "    df = None\n",
    "\n",
    "# If Option 1 fails, try Option 2: Load with specified engine\n",
    "if df is None:\n",
    "    print(\"\\nOption 2: Trying with python engine...\")\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            RAW_DATA_PATH,\n",
    "            sep=SEPARATOR,\n",
    "            encoding=FILE_ENCODING,\n",
    "            engine='python',\n",
    "            on_bad_lines='skip',\n",
    "            low_memory=False\n",
    "        )\n",
    "        print(f\"‚úì Successfully loaded with python engine\")\n",
    "        print(f\"  Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Failed: {e}\")\n",
    "        df = None\n",
    "\n",
    "# If still failing, try Option 3: Manual chunk loading\n",
    "if df is None:\n",
    "    print(\"\\nOption 3: Manual chunk loading with error handling...\")\n",
    "    \n",
    "    def load_chunks_manually(filepath, separator, encoding, chunk_size=50000):\n",
    "        chunks = []\n",
    "        problematic = []\n",
    "        \n",
    "        with open(filepath, 'r', encoding=encoding) as f:\n",
    "            # Read header\n",
    "            header = f.readline().strip()\n",
    "            expected_fields = len(header.split(separator))\n",
    "            print(f\"Expected fields based on header: {expected_fields}\")\n",
    "            \n",
    "            buffer = []\n",
    "            line_count = 0\n",
    "            \n",
    "            for i, line in enumerate(f, 2):  # Start from line 2 (after header)\n",
    "                line_count += 1\n",
    "                fields = line.strip().split(separator)\n",
    "                \n",
    "                if len(fields) == expected_fields:\n",
    "                    buffer.append(fields)\n",
    "                else:\n",
    "                    problematic.append((i, len(fields), line[:100]))\n",
    "                \n",
    "                # Process in chunks\n",
    "                if len(buffer) >= chunk_size:\n",
    "                    chunk_df = pd.DataFrame(buffer, columns=header.split(separator))\n",
    "                    chunks.append(chunk_df)\n",
    "                    buffer = []\n",
    "                    print(f\"  Processed {i:,} lines...\")\n",
    "            \n",
    "            # Process remaining buffer\n",
    "            if buffer:\n",
    "                chunk_df = pd.DataFrame(buffer, columns=header.split(separator))\n",
    "                chunks.append(chunk_df)\n",
    "        \n",
    "        # Combine chunks\n",
    "        if chunks:\n",
    "            final_df = pd.concat(chunks, ignore_index=True)\n",
    "        else:\n",
    "            final_df = pd.DataFrame(columns=header.split(separator))\n",
    "        \n",
    "        return final_df, problematic\n",
    "    \n",
    "    try:\n",
    "        df, problematic_lines = load_chunks_manually(\n",
    "            RAW_DATA_PATH, SEPARATOR, FILE_ENCODING, chunk_size=50000\n",
    "        )\n",
    "        print(f\"\\n‚úì Successfully loaded manually\")\n",
    "        print(f\"  Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "        print(f\"  Problematic lines skipped: {len(problematic_lines)}\")\n",
    "        \n",
    "        if problematic_lines:\n",
    "            print(\"\\nFirst 5 problematic lines:\")\n",
    "            for i, (line_num, field_count, line_preview) in enumerate(problematic_lines[:5], 1):\n",
    "                print(f\"  {i}. Line {line_num}: {field_count} fields (expected {len(df.columns)})\")\n",
    "                print(f\"     Preview: {line_preview}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Manual loading failed: {e}\")\n",
    "        print(\"\\nTrying one more approach...\")\n",
    "        \n",
    "        # Try reading as fixed width\n",
    "        try:\n",
    "            df = pd.read_fwf(RAW_DATA_PATH, encoding=FILE_ENCODING)\n",
    "            print(f\"‚úì Loaded as fixed width file\")\n",
    "            print(f\"  Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "        except:\n",
    "            print(\"‚úó All loading attempts failed\")\n",
    "            exit()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: BASIC DATA INSPECTION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BASIC DATA INSPECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\nColumn names:\")\n",
    "for i, col in enumerate(df.columns.tolist(), 1):\n",
    "    print(f\"{i:3}. {col}\")\n",
    "\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: VERIFY AGAINST PROJECT SPECIFICATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFICATION AGAINST PROJECT SPECIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "PROJECT_COLUMNS = [\n",
    "    # Insurance Policy\n",
    "    'UnderwrittenCoverID', 'PolicyID',\n",
    "    \n",
    "    # Transaction Date\n",
    "    'TransactionMonth',\n",
    "    \n",
    "    # Client Information\n",
    "    'IsVATRegistered', 'Citizenship', 'LegalType', 'Title', 'Language', 'Bank',\n",
    "    'AccountType', 'MaritalStatus', 'Gender',\n",
    "    \n",
    "    # Client Location\n",
    "    'Country', 'Province', 'PostalCode', 'MainCrestaZone', 'SubCrestaZone',\n",
    "    \n",
    "    # Car Insured\n",
    "    'ItemType', 'Mmcode', 'VehicleType', 'RegistrationYear', 'Make', 'Model',\n",
    "    'Cylinders', 'Cubiccapacity', 'Kilowatts', 'Bodytype', 'NumberOfDoors',\n",
    "    'VehicleIntroDate', 'CustomValueEstimate', 'AlarmImmobiliser', 'TrackingDevice',\n",
    "    'CapitalOutstanding', 'NewVehicle', 'WrittenOff', 'Rebuilt', 'Converted',\n",
    "    'CrossBorder', 'NumberOfVehiclesInFleet',\n",
    "    \n",
    "    # Insurance Plan\n",
    "    'SumInsured', 'TermFrequency', 'CalculatedPremiumPerTerm', 'ExcessSelected',\n",
    "    'CoverCategory', 'CoverType', 'CoverGroup', 'Section', 'Product',\n",
    "    'StatutoryClass', 'StatutoryRiskType',\n",
    "    \n",
    "    # Payment & Claim\n",
    "    'TotalPremium', 'TotalClaims'\n",
    "]\n",
    "\n",
    "print(f\"\\n‚úì Columns in project specification: {len(PROJECT_COLUMNS)}\")\n",
    "print(f\"‚úì Columns in loaded data: {len(df.columns)}\")\n",
    "\n",
    "# Check for matches\n",
    "matches = []\n",
    "missing = []\n",
    "extra = []\n",
    "\n",
    "for col in PROJECT_COLUMNS:\n",
    "    if col in df.columns:\n",
    "        matches.append(col)\n",
    "    else:\n",
    "        missing.append(col)\n",
    "\n",
    "for col in df.columns:\n",
    "    if col not in PROJECT_COLUMNS:\n",
    "        extra.append(col)\n",
    "\n",
    "print(f\"\\nüìä MATCH SUMMARY:\")\n",
    "print(f\"  ‚Ä¢ Matched columns: {len(matches)}\")\n",
    "print(f\"  ‚Ä¢ Missing columns: {len(missing)}\")\n",
    "print(f\"  ‚Ä¢ Extra columns: {len(extra)}\")\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\n‚ö†Ô∏è MISSING COLUMNS:\")\n",
    "    for i, col in enumerate(missing[:10], 1):  # Show first 10\n",
    "        print(f\"  {i}. {col}\")\n",
    "    if len(missing) > 10:\n",
    "        print(f\"  ... and {len(missing) - 10} more\")\n",
    "\n",
    "if extra:\n",
    "    print(f\"\\nüìù EXTRA COLUMNS (not in spec):\")\n",
    "    for i, col in enumerate(extra[:10], 1):\n",
    "        print(f\"  {i}. {col}\")\n",
    "    if len(extra) > 10:\n",
    "        print(f\"  ... and {len(extra) - 10} more\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: SAVE THE LOADED DATA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAVING LOADED DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create processed directory\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Save as parquet (efficient)\n",
    "processed_path = '../data/processed/raw_loaded_data.parquet'\n",
    "df.to_parquet(processed_path, index=False)\n",
    "print(f\"‚úì Saved loaded data to: {processed_path}\")\n",
    "print(f\"  Size: {os.path.getsize(processed_path) / 1024**2:.1f} MB\")\n",
    "\n",
    "# Save a sample for quick analysis\n",
    "sample_path = '../data/processed/data_sample.csv'\n",
    "df_sample = df.sample(min(10000, len(df)), random_state=42)\n",
    "df_sample.to_csv(sample_path, index=False)\n",
    "print(f\"‚úì Saved sample to: {sample_path}\")\n",
    "\n",
    "# Save verification metadata\n",
    "metadata = {\n",
    "    'loading_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'original_file': RAW_DATA_PATH,\n",
    "    'encoding': FILE_ENCODING,\n",
    "    'separator': repr(SEPARATOR),\n",
    "    'shape': list(df.shape),\n",
    "    'matches': len(matches),\n",
    "    'missing': missing,\n",
    "    'extra': extra,\n",
    "    'columns_loaded': df.columns.tolist()\n",
    "}\n",
    "\n",
    "metadata_path = '../data/processed/loading_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"‚úì Saved metadata to: {metadata_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: QUICK DATA QUALITY CHECK\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"QUICK DATA QUALITY CHECK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n1. Missing Values:\")\n",
    "missing_counts = df.isnull().sum()\n",
    "high_missing = missing_counts[missing_counts > 0]\n",
    "if len(high_missing) > 0:\n",
    "    for col, count in high_missing.head(10).items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        print(f\"  ‚Ä¢ {col}: {count:,} ({pct:.1f}%)\")\n",
    "else:\n",
    "    print(\"  ‚úì No missing values\")\n",
    "\n",
    "print(f\"\\n2. Basic Statistics:\")\n",
    "if 'TotalPremium' in df.columns and 'TotalClaims' in df.columns:\n",
    "    print(f\"  ‚Ä¢ Total Premium: R{df['TotalPremium'].sum():,.2f}\")\n",
    "    print(f\"  ‚Ä¢ Total Claims: R{df['TotalClaims'].sum():,.2f}\")\n",
    "    print(f\"  ‚Ä¢ Loss Ratio: {(df['TotalClaims'].sum() / df['TotalPremium'].sum() * 100):.1f}%\")\n",
    "\n",
    "print(f\"\\n3. Temporal Coverage:\")\n",
    "if 'TransactionMonth' in df.columns:\n",
    "    df['TransactionMonth'] = pd.to_datetime(df['TransactionMonth'], errors='coerce')\n",
    "    print(f\"  ‚Ä¢ Date Range: {df['TransactionMonth'].min()} to {df['TransactionMonth'].max()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: NEXT STEPS RECOMMENDATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NEXT STEPS RECOMMENDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nBased on the loaded data, here are the next steps:\")\n",
    "\n",
    "if len(matches) >= 40:  # If we have most columns\n",
    "    print(f\"1. ‚úÖ Data loaded successfully with {len(matches)}/{len(PROJECT_COLUMNS)} columns\")\n",
    "    print(f\"2. Proceed with preprocessing pipeline\")\n",
    "    print(f\"3. Focus on cleaning {len(missing)} missing columns\")\n",
    "elif len(matches) >= 20:\n",
    "    print(f\"1. ‚ö†Ô∏è Partial match: {len(matches)}/{len(PROJECT_COLUMNS)} columns\")\n",
    "    print(f\"2. Need to investigate column naming differences\")\n",
    "    print(f\"3. Check if extra columns can be mapped to missing ones\")\n",
    "else:\n",
    "    print(f\"1. ‚ùå Poor match: Only {len(matches)}/{len(PROJECT_COLUMNS)} columns\")\n",
    "    print(f\"2. Need to examine raw file structure more carefully\")\n",
    "    print(f\"3. Check if file format is correct\")\n",
    "\n",
    "print(f\"\\nüíæ Files created:\")\n",
    "print(f\"  ‚Ä¢ Processed data: {processed_path}\")\n",
    "print(f\"  ‚Ä¢ Sample data: {sample_path}\")\n",
    "print(f\"  ‚Ä¢ Loading metadata: {metadata_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"READY FOR PREPROCESSING\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE DATA PREPROCESSING PIPELINE - FIXED VERSION 2\n",
      "================================================================================\n",
      "\n",
      "=== STEP 1: COLUMN STANDARDIZATION ===\n",
      "‚úì Data loaded: 1,000,098 rows √ó 52 columns\n",
      "‚úì Column names standardized to match project specification\n",
      "‚úì All 52 project columns present\n",
      "\n",
      "=== STEP 2: DATA TYPE CONVERSION ===\n",
      "\n",
      "2.1 Date Conversions:\n",
      "  ‚Ä¢ TransactionMonth: 0 failed conversions (0.0%)\n",
      "  ‚Ä¢ VehicleIntroDate: 552 failed conversions (0.1%)\n",
      "\n",
      "2.2 Boolean Conversions:\n",
      "  ‚Ä¢ IsVATRegistered: 5,023 True values\n",
      "  ‚Ä¢ AlarmImmobiliser: 999,861 True values\n",
      "  ‚Ä¢ TrackingDevice: 343,481 True values\n",
      "\n",
      "2.3 Numeric Conversions:\n",
      "  ‚Ä¢ Cylinders: 552 null values (0.1%)\n",
      "  ‚Ä¢ Cubiccapacity: 552 null values (0.1%)\n",
      "  ‚Ä¢ Kilowatts: 552 null values (0.1%)\n",
      "  ‚Ä¢ NumberOfDoors: 552 null values (0.1%)\n",
      "  ‚Ä¢ CustomValueEstimate: 779,642 null values (78.0%)\n",
      "  ‚Ä¢ CapitalOutstanding: 322 null values (0.0%)\n",
      "  ‚Ä¢ NumberOfVehiclesInFleet: 1,000,098 null values (100.0%)\n",
      "  ‚Ä¢ ExcessSelected: 1,000,098 null values (100.0%)\n",
      "\n",
      "=== STEP 3: MISSING VALUE HANDLING ===\n",
      "\n",
      "3.1 Missing Value Analysis:\n",
      "  ‚Ä¢ High missing (>50%): 7 columns\n",
      "  ‚Ä¢ Medium missing (20-50%): 0 columns\n",
      "  ‚Ä¢ Low missing (‚â§20%): 16 columns\n",
      "\n",
      "  High missing columns (consider dropping):\n",
      "    - ExcessSelected: 100.0%\n",
      "    - NumberOfVehiclesInFleet: 100.0%\n",
      "    - CrossBorder: 99.9%\n",
      "    - CustomValueEstimate: 78.0%\n",
      "    - WrittenOff: 64.2%\n",
      "    - Rebuilt: 64.2%\n",
      "    - Converted: 64.2%\n",
      "\n",
      "3.2 Improved Imputation Strategy:\n",
      "  ‚Ä¢ Dropping columns with >80% missing: ['ExcessSelected', 'CrossBorder']\n",
      "  ‚Ä¢ Categorical columns: Impute with mode or 'Unknown'\n",
      "  ‚Ä¢ Numeric columns: Impute with median\n",
      "    - CustomValueEstimate: High missing, created imputation flag\n",
      "  ‚Ä¢ Boolean columns: Impute with False\n",
      "\n",
      "‚úì Missing values after imputation: 3,080,200\n",
      "\n",
      "=== STEP 4: FEATURE ENGINEERING ===\n",
      "\n",
      "4.1 Business Metrics:\n",
      "  ‚Ä¢ Created Loss_Ratio (Claims/Premium)\n",
      "  ‚Ä¢ Created Has_Claim flag: 0.3% of policies have claims\n",
      "  ‚Ä¢ Created Vehicle_Age\n",
      "\n",
      "4.2 Customer Segmentation:\n",
      "  ‚Ä¢ Created Fleet_Category\n",
      "  ‚Ä¢ Created Vehicle_Value_Category\n",
      "\n",
      "4.3 Risk Features:\n",
      "  ‚Ä¢ Created Security_Score (0-2)\n",
      "  ‚Ä¢ Created Premium_Per_1000\n",
      "\n",
      "4.4 Temporal Features:\n",
      "  ‚Ä¢ Created temporal features (Year, Month, Quarter, Is_Year_End)\n",
      "\n",
      "=== STEP 5: OUTLIER HANDLING ===\n",
      "\n",
      "5.1 Outlier Detection:\n",
      "  ‚Ä¢ TotalPremium        : 138,832 outliers (13.88%)\n",
      "  ‚Ä¢ TotalClaims         :  2,793 outliers (0.28%)\n",
      "  ‚Ä¢ CustomValueEstimate : 217,880 outliers (21.79%)\n",
      "  ‚Ä¢ SumInsured          : 104,166 outliers (10.42%)\n",
      "  ‚Ä¢ Premium_Per_1000    : 107,945 outliers (10.79%)\n",
      "\n",
      "5.2 Outlier Capping (Winsorizing):\n",
      "  ‚Ä¢ TotalPremium        : Capped at [-66, 88], mean change: 62 ‚Üí 21\n",
      "  ‚Ä¢ CustomValueEstimate : Capped at [220,000, 220,000], mean change: 221,219 ‚Üí 220,000\n",
      "  ‚Ä¢ SumInsured          : Capped at [0, 985,000], mean change: 604,173 ‚Üí 185,906\n",
      "  ‚Ä¢ Premium_Per_1000    : Capped at [0, 19], mean change: 250,868 ‚Üí 4\n",
      "\n",
      "=== STEP 6: DATA VALIDATION ===\n",
      "\n",
      "6.1 Business Logic Checks:\n",
      "  ‚ö†Ô∏è Claims > 120% SumInsured           : 840 records\n",
      "  ‚ö†Ô∏è Negative Premium                   : 288 records\n",
      "  ‚úÖ Unreasonable Vehicle Age           : 0 records\n",
      "  ‚ö†Ô∏è Extreme Loss Ratio (<0 or >500%)   : 2,547 records\n",
      "\n",
      "6.2 Data Quality Metrics:\n",
      "  ‚Ä¢ Total Records       : 1000098\n",
      "  ‚Ä¢ Total Columns       : 62\n",
      "  ‚Ä¢ Missing Values      : 3080200\n",
      "  ‚Ä¢ Missing Percentage  : 4.9676%\n",
      "  ‚Ä¢ Duplicate Rows      : 0\n",
      "  ‚Ä¢ Duplicate Percentage: 0.00%\n",
      "  ‚Ä¢ Memory Usage        : 1969.7 MB\n",
      "\n",
      "=== STEP 7: SAVING PROCESSED DATA ===\n",
      "‚úì Processed data saved: ../data/processed/insurance_data_processed.parquet\n",
      "  ‚Ä¢ Shape: 1,000,098 rows √ó 62 columns\n",
      "‚úì EDA sample saved: ../data/processed/insurance_data_sample_eda.csv (50,000 rows)\n",
      "\n",
      "=== STEP 8: SAVING METADATA ===\n",
      "‚úì Preprocessing metadata saved: ../data/processed/preprocessing_metadata.json\n",
      "\n",
      "================================================================================\n",
      "BUSINESS INSIGHTS PREVIEW\n",
      "================================================================================\n",
      "\n",
      "9.1 Key Financial Summary:\n",
      "  ‚Ä¢ Total Premium: R20,916,787.99\n",
      "  ‚Ä¢ Total Claims: R64,867,546.17\n",
      "  ‚Ä¢ Overall Loss Ratio: 310.1%\n",
      "  ‚Ä¢ Policies with Claims: 2,788 (0.3%)\n",
      "\n",
      "9.2 Risk Analysis by Province:\n",
      "  ‚Ä¢ Highest Risk Province: KwaZulu-Natal (Loss Ratio: 389.9%)\n",
      "  ‚Ä¢ Lowest Risk Province: Northern Cape (Loss Ratio: 66.2%)\n",
      "\n",
      "================================================================================\n",
      "HYPOTHESIS TESTING PREPARATION\n",
      "================================================================================\n",
      "\n",
      "‚Ä¢ Overall Profit: R-43,950,758.18\n",
      "‚Ä¢ Average Profit Margin: -25.0%\n",
      "\n",
      "‚úì Hypothesis testing data saved: ../data/processed/hypothesis_testing_data.parquet\n",
      "\n",
      "================================================================================\n",
      "PREPROCESSING PIPELINE COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üéØ KEY ACCOMPLISHMENTS:\n",
      "1. ‚úÖ Data loaded and standardized: 1M+ rows, 52 columns\n",
      "2. ‚úÖ Missing values handled: Strategic imputation applied\n",
      "3. ‚úÖ Outliers identified and capped: 5 key financial columns\n",
      "4. ‚úÖ Feature engineering: 10+ new business features created\n",
      "5. ‚úÖ Data validation: Business logic checks performed\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL BUSINESS FINDINGS:\n",
      "‚Ä¢ Overall Loss Ratio: 310.1% (OPERATING AT LOSS)\n",
      "‚Ä¢ Only 0.3% policies have claims\n",
      "‚Ä¢ High data quality issues: Multiple columns >50% missing\n",
      "\n",
      "üìä READY FOR ANALYSIS:\n",
      "1. A/B Testing: Provinces, Gender, Postal Codes\n",
      "2. Loss Ratio Analysis: By segment and geography\n",
      "3. Predictive Modeling: Premium optimization\n",
      "4. Risk Profiling: Identify low-risk segments\n",
      "\n",
      "üíæ OUTPUT FILES CREATED:\n",
      "   1. Processed Data: ../data/processed/insurance_data_processed.parquet\n",
      "   2. EDA Sample: ../data/processed/insurance_data_sample_eda.csv\n",
      "   3. Hypothesis Data: ../data/processed/hypothesis_testing_data.parquet\n",
      "   4. Metadata: ../data/processed/preprocessing_metadata.json\n",
      "\n",
      "üîú NEXT STEPS:\n",
      "   1. Perform detailed EDA with visualizations\n",
      "   2. Conduct statistical hypothesis tests\n",
      "   3. Build machine learning models\n",
      "   4. Create business recommendations\n",
      "\n",
      "================================================================================\n",
      "DATA READY FOR ALPHACARE INSURANCE SOLUTIONS ANALYSIS!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE DATA PREPROCESSING PIPELINE - FIXED VERSION 2\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom JSON encoder for numpy types\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, pd.Timestamp):\n",
    "            return str(obj)\n",
    "        if pd.isna(obj):\n",
    "            return None\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: INITIAL SETUP AND COLUMN STANDARDIZATION\n",
    "# ============================================================================\n",
    "print(\"\\n=== STEP 1: COLUMN STANDARDIZATION ===\")\n",
    "\n",
    "# Load the data we just saved\n",
    "df = pd.read_parquet('../data/processed/raw_loaded_data.parquet')\n",
    "print(f\"‚úì Data loaded: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "\n",
    "# Standardize column names to match project specification\n",
    "column_mapping = {\n",
    "    'mmcode': 'Mmcode',\n",
    "    'make': 'Make',\n",
    "    'cubiccapacity': 'Cubiccapacity',\n",
    "    'kilowatts': 'Kilowatts',\n",
    "    'bodytype': 'Bodytype'\n",
    "}\n",
    "\n",
    "df = df.rename(columns=column_mapping)\n",
    "print(\"‚úì Column names standardized to match project specification\")\n",
    "\n",
    "# Verify all columns are now present\n",
    "PROJECT_COLUMNS = [\n",
    "    'UnderwrittenCoverID', 'PolicyID', 'TransactionMonth', 'IsVATRegistered',\n",
    "    'Citizenship', 'LegalType', 'Title', 'Language', 'Bank', 'AccountType',\n",
    "    'MaritalStatus', 'Gender', 'Country', 'Province', 'PostalCode',\n",
    "    'MainCrestaZone', 'SubCrestaZone', 'ItemType', 'Mmcode', 'VehicleType',\n",
    "    'RegistrationYear', 'Make', 'Model', 'Cylinders', 'Cubiccapacity',\n",
    "    'Kilowatts', 'Bodytype', 'NumberOfDoors', 'VehicleIntroDate',\n",
    "    'CustomValueEstimate', 'AlarmImmobiliser', 'TrackingDevice',\n",
    "    'CapitalOutstanding', 'NewVehicle', 'WrittenOff', 'Rebuilt', 'Converted',\n",
    "    'CrossBorder', 'NumberOfVehiclesInFleet', 'SumInsured', 'TermFrequency',\n",
    "    'CalculatedPremiumPerTerm', 'ExcessSelected', 'CoverCategory', 'CoverType',\n",
    "    'CoverGroup', 'Section', 'Product', 'StatutoryClass', 'StatutoryRiskType',\n",
    "    'TotalPremium', 'TotalClaims'\n",
    "]\n",
    "\n",
    "missing = [col for col in PROJECT_COLUMNS if col not in df.columns]\n",
    "print(f\"‚úì All {len(PROJECT_COLUMNS)} project columns present\" if len(missing) == 0 else f\"Missing: {missing}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: DATA TYPE CONVERSION AND CLEANING\n",
    "# ============================================================================\n",
    "print(\"\\n=== STEP 2: DATA TYPE CONVERSION ===\")\n",
    "\n",
    "# Create a working copy\n",
    "df_clean = df.copy()\n",
    "\n",
    "# 2.1 Convert dates\n",
    "print(\"\\n2.1 Date Conversions:\")\n",
    "date_cols = ['TransactionMonth', 'VehicleIntroDate']\n",
    "for col in date_cols:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n",
    "        null_count = df_clean[col].isnull().sum()\n",
    "        print(f\"  ‚Ä¢ {col}: {null_count:,} failed conversions ({null_count/len(df_clean)*100:.1f}%)\")\n",
    "\n",
    "# 2.2 Convert boolean columns\n",
    "print(\"\\n2.2 Boolean Conversions:\")\n",
    "bool_cols = ['IsVATRegistered', 'AlarmImmobiliser', 'TrackingDevice']\n",
    "for col in bool_cols:\n",
    "    if col in df_clean.columns:\n",
    "        # Handle mixed types\n",
    "        df_clean[col] = df_clean[col].astype(str).str.lower().str.strip()\n",
    "        df_clean[col] = df_clean[col].map({\n",
    "            'true': True, 'yes': True, 'y': True, '1': True,\n",
    "            'false': False, 'no': False, 'n': False, '0': False\n",
    "        })\n",
    "        # Convert to boolean, NaN for unconvertible\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce').astype('boolean')\n",
    "        true_count = df_clean[col].sum()\n",
    "        print(f\"  ‚Ä¢ {col}: {true_count:,} True values\")\n",
    "\n",
    "# 2.3 Convert numeric columns\n",
    "print(\"\\n2.3 Numeric Conversions:\")\n",
    "numeric_cols = [\n",
    "    'RegistrationYear', 'Cylinders', 'Cubiccapacity', 'Kilowatts',\n",
    "    'NumberOfDoors', 'CustomValueEstimate', 'CapitalOutstanding',\n",
    "    'NumberOfVehiclesInFleet', 'SumInsured', 'CalculatedPremiumPerTerm',\n",
    "    'ExcessSelected', 'TotalPremium', 'TotalClaims'\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "        null_count = df_clean[col].isnull().sum()\n",
    "        if null_count > 0:\n",
    "            print(f\"  ‚Ä¢ {col}: {null_count:,} null values ({null_count/len(df_clean)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: HANDLE MISSING VALUES - IMPROVED STRATEGY\n",
    "# ============================================================================\n",
    "print(\"\\n=== STEP 3: MISSING VALUE HANDLING ===\")\n",
    "\n",
    "# 3.1 Analyze missing values\n",
    "missing_summary = df_clean.isnull().sum()\n",
    "missing_pct = (missing_summary / len(df_clean)) * 100\n",
    "\n",
    "print(\"\\n3.1 Missing Value Analysis:\")\n",
    "high_missing = missing_pct[missing_pct > 50].sort_values(ascending=False)\n",
    "medium_missing = missing_pct[(missing_pct > 20) & (missing_pct <= 50)].sort_values(ascending=False)\n",
    "low_missing = missing_pct[(missing_pct > 0) & (missing_pct <= 20)].sort_values(ascending=False)\n",
    "\n",
    "print(f\"  ‚Ä¢ High missing (>50%): {len(high_missing)} columns\")\n",
    "print(f\"  ‚Ä¢ Medium missing (20-50%): {len(medium_missing)} columns\")\n",
    "print(f\"  ‚Ä¢ Low missing (‚â§20%): {len(low_missing)} columns\")\n",
    "\n",
    "if len(high_missing) > 0:\n",
    "    print(\"\\n  High missing columns (consider dropping):\")\n",
    "    for col, pct in high_missing.items():\n",
    "        print(f\"    - {col}: {pct:.1f}%\")\n",
    "\n",
    "# 3.2 Improved imputation strategy\n",
    "print(\"\\n3.2 Improved Imputation Strategy:\")\n",
    "\n",
    "# Store which columns we're dropping\n",
    "cols_to_drop = []\n",
    "\n",
    "# Drop columns with >80% missing (not useful for analysis)\n",
    "cols_to_drop_candidates = high_missing[high_missing > 80].index.tolist()\n",
    "for col in cols_to_drop_candidates:\n",
    "    # Check if column is needed for feature engineering\n",
    "    if col not in ['NumberOfVehiclesInFleet']:  # Keep this for feature engineering\n",
    "        cols_to_drop.append(col)\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"  ‚Ä¢ Dropping columns with >80% missing: {cols_to_drop}\")\n",
    "    df_clean = df_clean.drop(columns=cols_to_drop)\n",
    "else:\n",
    "    print(\"  ‚Ä¢ No columns dropped (keeping all for feature engineering)\")\n",
    "\n",
    "# For categorical columns\n",
    "categorical_cols = [\n",
    "    'Citizenship', 'LegalType', 'Title', 'Language', 'Bank', 'AccountType',\n",
    "    'MaritalStatus', 'Gender', 'Country', 'Province', 'PostalCode',\n",
    "    'MainCrestaZone', 'SubCrestaZone', 'ItemType', 'VehicleType', 'Make',\n",
    "    'Model', 'Bodytype', 'CoverCategory', 'CoverType', 'CoverGroup',\n",
    "    'Section', 'Product', 'StatutoryClass', 'StatutoryRiskType'\n",
    "]\n",
    "\n",
    "print(\"  ‚Ä¢ Categorical columns: Impute with mode or 'Unknown'\")\n",
    "for col in categorical_cols:\n",
    "    if col in df_clean.columns and df_clean[col].isnull().any():\n",
    "        if col in ['Bank', 'AccountType']:\n",
    "            df_clean[col] = df_clean[col].fillna('Not Specified')\n",
    "        else:\n",
    "            mode_val = df_clean[col].mode()[0] if not df_clean[col].mode().empty else 'Unknown'\n",
    "            df_clean[col] = df_clean[col].fillna(mode_val)\n",
    "\n",
    "# For numeric columns, use median but log when imputing high missing\n",
    "print(\"  ‚Ä¢ Numeric columns: Impute with median\")\n",
    "for col in numeric_cols:\n",
    "    if col in df_clean.columns and df_clean[col].isnull().any():\n",
    "        if col in ['CustomValueEstimate'] and df_clean[col].isnull().mean() > 0.5:\n",
    "            # Create flag for imputed values\n",
    "            df_clean[f'{col}_Imputed'] = df_clean[col].isnull().astype(int)\n",
    "            print(f\"    - {col}: High missing, created imputation flag\")\n",
    "        \n",
    "        median_val = df_clean[col].median()\n",
    "        df_clean[col] = df_clean[col].fillna(median_val)\n",
    "\n",
    "# For boolean columns\n",
    "print(\"  ‚Ä¢ Boolean columns: Impute with False\")\n",
    "for col in bool_cols:\n",
    "    if col in df_clean.columns and df_clean[col].isnull().any():\n",
    "        df_clean[col] = df_clean[col].fillna(False)\n",
    "\n",
    "print(f\"\\n‚úì Missing values after imputation: {df_clean.isnull().sum().sum():,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: FEATURE ENGINEERING FOR RISK ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n=== STEP 4: FEATURE ENGINEERING ===\")\n",
    "\n",
    "# 4.1 Create key business metrics\n",
    "print(\"\\n4.1 Business Metrics:\")\n",
    "\n",
    "# Loss Ratio (critical for insurance)\n",
    "df_clean['Loss_Ratio'] = df_clean['TotalClaims'] / df_clean['TotalPremium']\n",
    "df_clean['Loss_Ratio'] = df_clean['Loss_Ratio'].replace([np.inf, -np.inf], np.nan)\n",
    "df_clean['Loss_Ratio'] = df_clean['Loss_Ratio'].fillna(0)\n",
    "print(f\"  ‚Ä¢ Created Loss_Ratio (Claims/Premium)\")\n",
    "\n",
    "# Claim indicator\n",
    "df_clean['Has_Claim'] = (df_clean['TotalClaims'] > 0).astype(int)\n",
    "claim_rate = df_clean['Has_Claim'].mean() * 100\n",
    "print(f\"  ‚Ä¢ Created Has_Claim flag: {claim_rate:.1f}% of policies have claims\")\n",
    "\n",
    "# Vehicle age\n",
    "current_year = 2015  # Based on data end date\n",
    "df_clean['Vehicle_Age'] = current_year - df_clean['RegistrationYear']\n",
    "df_clean['Vehicle_Age'] = df_clean['Vehicle_Age'].apply(lambda x: x if 0 <= x <= 50 else np.nan)\n",
    "print(f\"  ‚Ä¢ Created Vehicle_Age\")\n",
    "\n",
    "# 4.2 Customer segmentation\n",
    "print(\"\\n4.2 Customer Segmentation:\")\n",
    "\n",
    "# Fleet size categories - handle missing column\n",
    "if 'NumberOfVehiclesInFleet' in df_clean.columns:\n",
    "    def categorize_fleet(x):\n",
    "        if pd.isna(x) or x == 0:\n",
    "            return 'Unknown'\n",
    "        elif x == 1:\n",
    "            return 'Individual'\n",
    "        elif 2 <= x <= 5:\n",
    "            return 'Small Fleet'\n",
    "        elif 6 <= x <= 20:\n",
    "            return 'Medium Fleet'\n",
    "        else:\n",
    "            return 'Large Fleet'\n",
    "    \n",
    "    df_clean['Fleet_Category'] = df_clean['NumberOfVehiclesInFleet'].apply(categorize_fleet)\n",
    "    print(f\"  ‚Ä¢ Created Fleet_Category\")\n",
    "else:\n",
    "    print(f\"  ‚Ä¢ Skipping Fleet_Category - column not available\")\n",
    "\n",
    "# Vehicle value categories (in South African Rands)\n",
    "def categorize_value(x):\n",
    "    if pd.isna(x):\n",
    "        return 'Unknown'\n",
    "    elif x < 50000:\n",
    "        return 'Budget (<R50k)'\n",
    "    elif 50000 <= x < 150000:\n",
    "        return 'Economy (R50k-150k)'\n",
    "    elif 150000 <= x < 300000:\n",
    "        return 'Standard (R150k-300k)'\n",
    "    elif 300000 <= x < 600000:\n",
    "        return 'Premium (R300k-600k)'\n",
    "    else:\n",
    "        return 'Luxury (>R600k)'\n",
    "\n",
    "df_clean['Vehicle_Value_Category'] = df_clean['CustomValueEstimate'].apply(categorize_value)\n",
    "print(f\"  ‚Ä¢ Created Vehicle_Value_Category\")\n",
    "\n",
    "# 4.3 Risk-related features\n",
    "print(\"\\n4.3 Risk Features:\")\n",
    "\n",
    "# Security score\n",
    "df_clean['Security_Score'] = df_clean['AlarmImmobiliser'].astype(int) + df_clean['TrackingDevice'].astype(int)\n",
    "print(f\"  ‚Ä¢ Created Security_Score (0-2)\")\n",
    "\n",
    "# Premium adequacy (premium per R1000 of sum insured)\n",
    "df_clean['Premium_Per_1000'] = (df_clean['CalculatedPremiumPerTerm'] / df_clean['SumInsured']) * 1000\n",
    "df_clean['Premium_Per_1000'] = df_clean['Premium_Per_1000'].replace([np.inf, -np.inf], np.nan)\n",
    "df_clean['Premium_Per_1000'] = df_clean['Premium_Per_1000'].fillna(df_clean['Premium_Per_1000'].median())\n",
    "print(f\"  ‚Ä¢ Created Premium_Per_1000\")\n",
    "\n",
    "# 4.4 Temporal features\n",
    "print(\"\\n4.4 Temporal Features:\")\n",
    "\n",
    "if 'TransactionMonth' in df_clean.columns:\n",
    "    df_clean['Transaction_Year'] = df_clean['TransactionMonth'].dt.year\n",
    "    df_clean['Transaction_Month'] = df_clean['TransactionMonth'].dt.month\n",
    "    df_clean['Transaction_Quarter'] = df_clean['TransactionMonth'].dt.quarter\n",
    "    df_clean['Is_Year_End'] = df_clean['Transaction_Month'].isin([11, 12]).astype(int)\n",
    "    print(f\"  ‚Ä¢ Created temporal features (Year, Month, Quarter, Is_Year_End)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: OUTLIER DETECTION AND HANDLING\n",
    "# ============================================================================\n",
    "print(\"\\n=== STEP 5: OUTLIER HANDLING ===\")\n",
    "\n",
    "# 5.1 Identify outliers in key financial columns\n",
    "key_columns = ['TotalPremium', 'TotalClaims', 'CustomValueEstimate', 'SumInsured', 'Premium_Per_1000']\n",
    "\n",
    "print(\"\\n5.1 Outlier Detection:\")\n",
    "outlier_summary = {}\n",
    "for col in key_columns:\n",
    "    if col in df_clean.columns:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 3 * IQR\n",
    "        upper_bound = Q3 + 3 * IQR\n",
    "        \n",
    "        outliers = ((df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)).sum()\n",
    "        outlier_pct = (outliers / len(df_clean)) * 100\n",
    "        \n",
    "        outlier_summary[col] = {\n",
    "            'outliers': int(outliers),\n",
    "            'pct': float(outlier_pct),\n",
    "            'lower': float(lower_bound),\n",
    "            'upper': float(upper_bound)\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚Ä¢ {col:20}: {outliers:6,} outliers ({outlier_pct:.2f}%)\")\n",
    "\n",
    "# 5.2 Cap outliers (Winsorizing)\n",
    "print(\"\\n5.2 Outlier Capping (Winsorizing):\")\n",
    "for col, stats in outlier_summary.items():\n",
    "    if stats['pct'] > 1:  # Cap if more than 1% outliers\n",
    "        lower_bound = max(stats['lower'], df_clean[col].min())  # Don't go below actual min\n",
    "        upper_bound = min(stats['upper'], df_clean[col].max() * 0.99)  # Keep some high values\n",
    "        \n",
    "        before_mean = float(df_clean[col].mean())\n",
    "        df_clean[col] = np.where(df_clean[col] < lower_bound, lower_bound, df_clean[col])\n",
    "        df_clean[col] = np.where(df_clean[col] > upper_bound, upper_bound, df_clean[col])\n",
    "        after_mean = float(df_clean[col].mean())\n",
    "        \n",
    "        print(f\"  ‚Ä¢ {col:20}: Capped at [{lower_bound:,.0f}, {upper_bound:,.0f}], \"\n",
    "              f\"mean change: {before_mean:,.0f} ‚Üí {after_mean:,.0f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: DATA VALIDATION AND QUALITY CHECKS\n",
    "# ============================================================================\n",
    "print(\"\\n=== STEP 6: DATA VALIDATION ===\")\n",
    "\n",
    "# 6.1 Business logic validation\n",
    "print(\"\\n6.1 Business Logic Checks:\")\n",
    "\n",
    "validation_issues = []\n",
    "\n",
    "# Check 1: Claims shouldn't exceed sum insured (allowing 20% buffer)\n",
    "if all(col in df_clean.columns for col in ['TotalClaims', 'SumInsured']):\n",
    "    invalid_claims = df_clean[df_clean['TotalClaims'] > df_clean['SumInsured'] * 1.2]\n",
    "    validation_issues.append(('Claims > 120% SumInsured', int(len(invalid_claims))))\n",
    "\n",
    "# Check 2: Premium should be positive (allow zero for lapsed policies)\n",
    "if 'TotalPremium' in df_clean.columns:\n",
    "    negative_premium = df_clean[df_clean['TotalPremium'] < 0]\n",
    "    validation_issues.append(('Negative Premium', int(len(negative_premium))))\n",
    "\n",
    "# Check 3: Vehicle age should be reasonable\n",
    "if 'Vehicle_Age' in df_clean.columns:\n",
    "    unreasonable_age = df_clean[(df_clean['Vehicle_Age'] < 0) | (df_clean['Vehicle_Age'] > 50)]\n",
    "    validation_issues.append(('Unreasonable Vehicle Age', int(len(unreasonable_age))))\n",
    "\n",
    "# Check 4: Loss ratio should be reasonable (0-500%)\n",
    "if 'Loss_Ratio' in df_clean.columns:\n",
    "    extreme_loss_ratio = df_clean[(df_clean['Loss_Ratio'] < 0) | (df_clean['Loss_Ratio'] > 5)]\n",
    "    validation_issues.append(('Extreme Loss Ratio (<0 or >500%)', int(len(extreme_loss_ratio))))\n",
    "\n",
    "for issue, count in validation_issues:\n",
    "    status = \"‚ö†Ô∏è \" if count > 0 else \"‚úÖ \"\n",
    "    print(f\"  {status}{issue:35}: {count:,} records\")\n",
    "\n",
    "# 6.2 Data quality metrics\n",
    "print(\"\\n6.2 Data Quality Metrics:\")\n",
    "quality_metrics = {\n",
    "    'Total Records': int(len(df_clean)),\n",
    "    'Total Columns': int(len(df_clean.columns)),\n",
    "    'Missing Values': int(df_clean.isnull().sum().sum()),\n",
    "    'Missing Percentage': f\"{(df_clean.isnull().sum().sum() / (len(df_clean) * len(df_clean.columns))) * 100:.4f}%\",\n",
    "    'Duplicate Rows': int(df_clean.duplicated().sum()),\n",
    "    'Duplicate Percentage': f\"{(df_clean.duplicated().sum() / len(df_clean)) * 100:.2f}%\",\n",
    "    'Memory Usage': f\"{df_clean.memory_usage(deep=True).sum() / 1024**2:.1f} MB\"\n",
    "}\n",
    "\n",
    "for metric, value in quality_metrics.items():\n",
    "    print(f\"  ‚Ä¢ {metric:20}: {value}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: SAVE PROCESSED DATA\n",
    "# ============================================================================\n",
    "print(\"\\n=== STEP 7: SAVING PROCESSED DATA ===\")\n",
    "\n",
    "# Ensure processed directory exists\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Save processed data\n",
    "output_path = '../data/processed/insurance_data_processed.parquet'\n",
    "df_clean.to_parquet(output_path, index=False)\n",
    "print(f\"‚úì Processed data saved: {output_path}\")\n",
    "print(f\"  ‚Ä¢ Shape: {df_clean.shape[0]:,} rows √ó {df_clean.shape[1]} columns\")\n",
    "\n",
    "# Save a sample for EDA\n",
    "sample_path = '../data/processed/insurance_data_sample_eda.csv'\n",
    "sample_size = min(50000, len(df_clean))\n",
    "df_sample = df_clean.sample(n=sample_size, random_state=42)\n",
    "df_sample.to_csv(sample_path, index=False)\n",
    "print(f\"‚úì EDA sample saved: {sample_path} ({sample_size:,} rows)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: SAVE PREPROCESSING METADATA\n",
    "# ============================================================================\n",
    "print(\"\\n=== STEP 8: SAVING METADATA ===\")\n",
    "\n",
    "# Save preprocessing metadata\n",
    "preprocessing_metadata = {\n",
    "    'preprocessing_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'original_shape': [int(df.shape[0]), int(df.shape[1])],\n",
    "    'processed_shape': [int(df_clean.shape[0]), int(df_clean.shape[1])],\n",
    "    'columns_standardized': list(column_mapping.keys()),\n",
    "    'columns_dropped': cols_to_drop,\n",
    "    'new_features_created': [\n",
    "        'Loss_Ratio', 'Has_Claim', 'Vehicle_Age', 'Vehicle_Value_Category',\n",
    "        'Security_Score', 'Premium_Per_1000',\n",
    "        'Transaction_Year', 'Transaction_Month', 'Transaction_Quarter', 'Is_Year_End'\n",
    "    ] + (['Fleet_Category'] if 'Fleet_Category' in df_clean.columns else []),\n",
    "    'outlier_handling': outlier_summary,\n",
    "    'validation_issues': dict(validation_issues),\n",
    "    'quality_metrics': quality_metrics\n",
    "}\n",
    "\n",
    "metadata_path = '../data/processed/preprocessing_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(preprocessing_metadata, f, cls=NumpyEncoder, indent=2)\n",
    "print(f\"‚úì Preprocessing metadata saved: {metadata_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 9: BUSINESS INSIGHTS PREVIEW\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BUSINESS INSIGHTS PREVIEW\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n9.1 Key Financial Summary:\")\n",
    "if all(col in df_clean.columns for col in ['TotalPremium', 'TotalClaims']):\n",
    "    total_premium = float(df_clean['TotalPremium'].sum())\n",
    "    total_claims = float(df_clean['TotalClaims'].sum())\n",
    "    overall_loss_ratio = (total_claims / total_premium) * 100 if total_premium > 0 else 0\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Total Premium: R{total_premium:,.2f}\")\n",
    "    print(f\"  ‚Ä¢ Total Claims: R{total_claims:,.2f}\")\n",
    "    print(f\"  ‚Ä¢ Overall Loss Ratio: {overall_loss_ratio:.1f}%\")\n",
    "    print(f\"  ‚Ä¢ Policies with Claims: {int(df_clean['Has_Claim'].sum()):,} ({df_clean['Has_Claim'].mean()*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n9.2 Risk Analysis by Province:\")\n",
    "if all(col in df_clean.columns for col in ['Province', 'Loss_Ratio']):\n",
    "    province_risk = df_clean.groupby('Province').agg({\n",
    "        'TotalPremium': 'sum',\n",
    "        'TotalClaims': 'sum',\n",
    "        'Has_Claim': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    province_risk['Loss_Ratio'] = (province_risk['TotalClaims'] / province_risk['TotalPremium']) * 100\n",
    "    province_risk = province_risk.sort_values('Loss_Ratio', ascending=False)\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Highest Risk Province: {province_risk.iloc[0]['Province']} \"\n",
    "          f\"(Loss Ratio: {province_risk.iloc[0]['Loss_Ratio']:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Lowest Risk Province: {province_risk.iloc[-1]['Province']} \"\n",
    "          f\"(Loss Ratio: {province_risk.iloc[-1]['Loss_Ratio']:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 10: PREPARE FOR HYPOTHESIS TESTING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HYPOTHESIS TESTING PREPARATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create profit metrics\n",
    "df_clean['Profit'] = df_clean['TotalPremium'] - df_clean['TotalClaims']\n",
    "df_clean['Profit_Margin'] = (df_clean['Profit'] / df_clean['TotalPremium']) * 100\n",
    "df_clean['Profit_Margin'] = df_clean['Profit_Margin'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "print(f\"\\n‚Ä¢ Overall Profit: R{df_clean['Profit'].sum():,.2f}\")\n",
    "print(f\"‚Ä¢ Average Profit Margin: {df_clean['Profit_Margin'].mean():.1f}%\")\n",
    "\n",
    "# Save hypothesis testing data\n",
    "hypothesis_path = '../data/processed/hypothesis_testing_data.parquet'\n",
    "df_clean.to_parquet(hypothesis_path, index=False)\n",
    "print(f\"\\n‚úì Hypothesis testing data saved: {hypothesis_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREPROCESSING PIPELINE COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüéØ KEY ACCOMPLISHMENTS:\")\n",
    "print(f\"1. ‚úÖ Data loaded and standardized: 1M+ rows, 52 columns\")\n",
    "print(f\"2. ‚úÖ Missing values handled: Strategic imputation applied\")\n",
    "print(f\"3. ‚úÖ Outliers identified and capped: 5 key financial columns\")\n",
    "print(f\"4. ‚úÖ Feature engineering: 10+ new business features created\")\n",
    "print(f\"5. ‚úÖ Data validation: Business logic checks performed\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  CRITICAL BUSINESS FINDINGS:\")\n",
    "print(f\"‚Ä¢ Overall Loss Ratio: {overall_loss_ratio:.1f}% (OPERATING AT LOSS)\")\n",
    "print(f\"‚Ä¢ Only {df_clean['Has_Claim'].mean()*100:.1f}% policies have claims\")\n",
    "print(f\"‚Ä¢ High data quality issues: Multiple columns >50% missing\")\n",
    "\n",
    "print(f\"\\nüìä READY FOR ANALYSIS:\")\n",
    "print(f\"1. A/B Testing: Provinces, Gender, Postal Codes\")\n",
    "print(f\"2. Loss Ratio Analysis: By segment and geography\")\n",
    "print(f\"3. Predictive Modeling: Premium optimization\")\n",
    "print(f\"4. Risk Profiling: Identify low-risk segments\")\n",
    "\n",
    "print(f\"\\nüíæ OUTPUT FILES CREATED:\")\n",
    "print(f\"   1. Processed Data: {output_path}\")\n",
    "print(f\"   2. EDA Sample: {sample_path}\")\n",
    "print(f\"   3. Hypothesis Data: {hypothesis_path}\")\n",
    "print(f\"   4. Metadata: {metadata_path}\")\n",
    "\n",
    "print(f\"\\nüîú NEXT STEPS:\")\n",
    "print(f\"   1. Perform detailed EDA with visualizations\")\n",
    "print(f\"   2. Conduct statistical hypothesis tests\")\n",
    "print(f\"   3. Build machine learning models\")\n",
    "print(f\"   4. Create business recommendations\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA READY FOR ALPHACARE INSURANCE SOLUTIONS ANALYSIS!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
